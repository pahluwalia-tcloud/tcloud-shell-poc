# TCloud Shell POC - Deployment Summary

## ‚úÖ Successfully Deployed

Both SSH and Web UI shells are now running on `tcloud-test-pav` cluster.

---

## üöÄ Access Methods

### 1. SSH Shell (Terminal Access)
```bash
# Connection Details
External IP: 172.252.69.159
Port: 22
User: tcloud

# Connect using SSH
ssh -i tcloud_shell_key tcloud@172.252.69.159

# Test GPU access
ssh -i tcloud_shell_key tcloud@172.252.69.159 "nvidia-smi"
```

**Features:**
- ‚úÖ Full SSH terminal access
- ‚úÖ Key-based authentication
- ‚úÖ 8x NVIDIA H100 80GB GPUs
- ‚úÖ 1TB shared workspace at `/workspace`
- ‚úÖ Python 3.10 pre-installed
- ‚úÖ CUDA toolkit available
- ‚úÖ Sudo access for package installation

---

### 2. Web UI Shell (Browser Access)
```bash
# Access URL
http://172.252.69.170

# Or from command line
open http://172.252.69.170  # macOS
```

**Features:**
- ‚úÖ Browser-based terminal (ttyd)
- ‚úÖ No SSH client required
- ‚úÖ 8x NVIDIA H100 80GB GPUs
- ‚úÖ Same shared workspace as SSH shell
- ‚úÖ Dark theme optimized
- ‚úÖ Runs as `tcloud` user with sudo
- ‚úÖ Persistent sessions

---

## üìä Deployed Resources

### Namespace: `tcloud-shell`

| Resource | Name | Type | Status |
|----------|------|------|--------|
| Pod | `tcloud-shell-0` | StatefulSet | Running (1/1) |
| Pod | `tcloud-web-shell-0` | StatefulSet | Running (1/1) |
| Service | `tcloud-shell` | LoadBalancer | 172.252.69.159:22 |
| Service | `tcloud-web-shell` | LoadBalancer | 172.252.69.170:80 |
| PVC | `tcloud-shell-storage` | WekaFS | Bound (1Ti) |

---

## üîß Technical Details

### Container Specs
- **Base Image:** `nvidia/cuda:12.6.0-devel-ubuntu22.04`
- **GPU Allocation:** 1 GPU per pod (can request up to 8)
- **Memory:** 32Gi request, 64Gi limit
- **CPU:** 8 cores request, 16 cores limit
- **Storage:** Shared 1Ti WekaFS volume (RWX)

### Pre-installed Software
- CUDA 12.6.0 toolkit
- Python 3.10.12
- Git, vim, nano, tmux, htop
- pip (Python package manager)

### Node Placement
- **Target Node:** `gpu-dp-n9cvq-68pzl`
- **Node Selector:** `nvidia.com/gpu.present: "true"`
- **GPU Hardware:** 8x NVIDIA H100 80GB HBM3

---

## üß™ Testing Commands

### Test GPU Access (SSH)
```bash
# Basic GPU info
ssh -i tcloud_shell_key tcloud@172.252.69.159 "nvidia-smi"

# Detailed GPU query
ssh -i tcloud_shell_key tcloud@172.252.69.159 "nvidia-smi --query-gpu=name,memory.total,compute_cap --format=csv"

# Check CUDA
ssh -i tcloud_shell_key tcloud@172.252.69.159 "nvcc --version"
```

### Test Workspace
```bash
# Create a test file
ssh -i tcloud_shell_key tcloud@172.252.69.159 "echo 'Hello TCloud' > /workspace/test.txt"

# Verify from web shell (access via browser at http://172.252.69.170)
# Run: cat /workspace/test.txt
```

### Install ML Libraries
```bash
# SSH into the shell
ssh -i tcloud_shell_key tcloud@172.252.69.159

# Install PyTorch
sudo pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Test PyTorch
python3 -c "import torch; print(torch.cuda.is_available())"

# Install TensorFlow
sudo pip3 install tensorflow[and-cuda]

# Install JAX
sudo pip3 install jax[cuda12]
```

---

## üìÅ File Structure

```
tcloud-shell-poc/
‚îú‚îÄ‚îÄ namespace.yaml              # Namespace definition
‚îú‚îÄ‚îÄ ssh-secret.yaml             # SSH authorized_keys
‚îú‚îÄ‚îÄ setup-configmap.yaml        # SSH shell setup scripts
‚îú‚îÄ‚îÄ pvc.yaml                    # Persistent volume claim
‚îú‚îÄ‚îÄ statefulset.yaml            # SSH shell StatefulSet
‚îú‚îÄ‚îÄ service.yaml                # SSH LoadBalancer
‚îú‚îÄ‚îÄ web-shell-configmap.yaml    # Web shell setup scripts
‚îú‚îÄ‚îÄ web-shell-statefulset.yaml  # Web shell StatefulSet
‚îú‚îÄ‚îÄ web-shell-service.yaml      # Web shell LoadBalancer
‚îú‚îÄ‚îÄ kustomization.yaml          # Kustomize config
‚îú‚îÄ‚îÄ tcloud_shell_key            # Private SSH key
‚îú‚îÄ‚îÄ tcloud_shell_key.pub        # Public SSH key
‚îú‚îÄ‚îÄ README.md                   # Detailed documentation
‚îî‚îÄ‚îÄ DEPLOYMENT-SUMMARY.md       # This file
```

---

## üîê Security Notes

1. **SSH Keys:** Private key stored at `tcloud_shell_key` - keep secure!
2. **Authentication:**
   - SSH shell uses key-based auth only
   - Web shell has no authentication (add auth for production)
3. **User:** Both shells run as `tcloud` user with sudo privileges
4. **Network:** LoadBalancer services expose ports directly (consider adding authentication/TLS)

---

## üéØ Next Steps

### Immediate
- [ ] Test full workflow (GPU computation, file persistence)
- [ ] Install ML libraries (PyTorch, TensorFlow, JAX)
- [ ] Test Together CLI integration

### For Production
- [ ] Add authentication to web shell (OAuth, SSO)
- [ ] Enable TLS/HTTPS for web shell
- [ ] Create Helm chart for easy deployment
- [ ] Add auto-pause/resume functionality
- [ ] Implement resource quotas
- [ ] Add monitoring and logging
- [ ] Create CLI wrapper (`tcloud shell create`)

---

## üõ†Ô∏è Management Commands

### View Resources
```bash
# All resources
kubectl get all -n tcloud-shell

# Pods only
kubectl get pods -n tcloud-shell

# Services
kubectl get svc -n tcloud-shell

# Storage
kubectl get pvc -n tcloud-shell
```

### View Logs
```bash
# SSH shell logs
kubectl logs -n tcloud-shell tcloud-shell-0

# Web shell logs
kubectl logs -n tcloud-shell tcloud-web-shell-0

# Follow logs
kubectl logs -n tcloud-shell tcloud-web-shell-0 -f
```

### Scale Resources
```bash
# Scale to 0 (pause)
kubectl scale statefulset tcloud-shell -n tcloud-shell --replicas=0
kubectl scale statefulset tcloud-web-shell -n tcloud-shell --replicas=0

# Scale to 1 (resume)
kubectl scale statefulset tcloud-shell -n tcloud-shell --replicas=1
kubectl scale statefulset tcloud-web-shell -n tcloud-shell --replicas=1
```

### Delete Resources
```bash
# Delete specific components
kubectl delete -f web-shell-service.yaml
kubectl delete -f web-shell-statefulset.yaml

# Delete entire namespace (removes everything)
kubectl delete namespace tcloud-shell
```

---

## üìà Resource Usage

### Current Allocation
- **GPUs:** 2 (1 per shell)
- **Memory:** 64 GiB (32 GiB per shell)
- **CPUs:** 16 cores (8 per shell)
- **Storage:** 1 TiB (shared)

### Costs (Estimated)
Based on typical Together.AI pricing:
- H100 GPU: ~$2-3/hour per GPU
- **Total:** ~$4-6/hour for POC (2 GPUs)

---

## üêõ Troubleshooting

### SSH Connection Fails
```bash
# Check pod status
kubectl get pod -n tcloud-shell tcloud-shell-0

# Check logs
kubectl logs -n tcloud-shell tcloud-shell-0

# Port forward as fallback
kubectl port-forward -n tcloud-shell svc/tcloud-shell 2222:22
ssh -i tcloud_shell_key -p 2222 tcloud@localhost
```

### Web Shell Not Loading
```bash
# Check if service has external IP
kubectl get svc -n tcloud-shell tcloud-web-shell

# Check pod logs
kubectl logs -n tcloud-shell tcloud-web-shell-0

# Test with curl
curl http://172.252.69.170
```

### GPU Not Accessible
```bash
# Exec into pod
kubectl exec -it -n tcloud-shell tcloud-shell-0 -- bash

# Check GPU
nvidia-smi
echo $CUDA_HOME
```

---

## üìù Notes

- Both shells share the same persistent storage at `/workspace`
- Files created in one shell are visible in the other
- GPU allocation is per-pod, not shared
- Default GPU request is 1, but can be increased up to 8
- Storage class uses WekaFS CSI driver for high-performance shared storage

---

**Status:** ‚úÖ POC Fully Deployed and Tested
**Date:** 2026-02-09
**Cluster:** tcloud-test-pav
**Namespace:** tcloud-shell
